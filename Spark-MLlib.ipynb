{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SentimentAnalysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+--------------------+\n",
      "|TweetID|     Entity|Sentiment|             Content|\n",
      "+-------+-----------+---------+--------------------+\n",
      "|   2401|Borderlands| Positive|im getting on bor...|\n",
      "|   2401|Borderlands| Positive|I am coming to th...|\n",
      "|   2401|Borderlands| Positive|im getting on bor...|\n",
      "|   2401|Borderlands| Positive|im coming on bord...|\n",
      "|   2401|Borderlands| Positive|im getting on bor...|\n",
      "|   2401|Borderlands| Positive|im getting into b...|\n",
      "|   2402|Borderlands| Positive|So I spent a few ...|\n",
      "|   2402|Borderlands| Positive|So I spent a coup...|\n",
      "|   2402|Borderlands| Positive|So I spent a few ...|\n",
      "|   2402|Borderlands| Positive|So I spent a few ...|\n",
      "|   2402|Borderlands| Positive|2010 So I spent a...|\n",
      "|   2402|Borderlands| Positive|                 was|\n",
      "|   2403|Borderlands|  Neutral|Rock-Hard La Varl...|\n",
      "|   2403|Borderlands|  Neutral|Rock-Hard La Varl...|\n",
      "|   2403|Borderlands|  Neutral|Rock-Hard La Varl...|\n",
      "|   2403|Borderlands|  Neutral|Rock-Hard La Vita...|\n",
      "|   2403|Borderlands|  Neutral|Live Rock - Hard ...|\n",
      "|   2403|Borderlands|  Neutral|I-Hard like me, R...|\n",
      "|   2404|Borderlands| Positive|that was the firs...|\n",
      "|   2404|Borderlands| Positive|this was the firs...|\n",
      "+-------+-----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"TweetID\", IntegerType(), True),\n",
    "    StructField(\"Entity\", StringType(), True),\n",
    "    StructField(\"Sentiment\", StringType(), True),\n",
    "    StructField(\"Content\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"./Spark/twitter_training.csv\", header=False, schema=schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.filter(df[\"Content\"].isNotNull())\n",
    "df = df.fillna({\"Content\": \"default_value\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|TweetID|     Entity|Sentiment|             Content|               words|            filtered|         rawFeatures|            features|\n",
      "+-------+-----------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   2401|Borderlands| Positive|im getting on bor...|[im, getting, on,...|[im, getting, bor...|(262144,[31015,92...|(262144,[31015,92...|\n",
      "|   2401|Borderlands| Positive|I am coming to th...|[i, am, coming, t...|[coming, borders,...|(262144,[12409,14...|(262144,[12409,14...|\n",
      "|   2401|Borderlands| Positive|im getting on bor...|[im, getting, on,...|[im, getting, bor...|(262144,[31015,68...|(262144,[31015,68...|\n",
      "|   2401|Borderlands| Positive|im coming on bord...|[im, coming, on, ...|[im, coming, bord...|(262144,[12409,31...|(262144,[12409,31...|\n",
      "|   2401|Borderlands| Positive|im getting on bor...|[im, getting, on,...|[im, getting, bor...|(262144,[12524,31...|(262144,[12524,31...|\n",
      "|   2401|Borderlands| Positive|im getting into b...|[im, getting, int...|[im, getting, bor...|(262144,[31015,68...|(262144,[31015,68...|\n",
      "|   2402|Borderlands| Positive|So I spent a few ...|[so, i, spent, a,...|[spent, hours, ma...|(262144,[640,2182...|(262144,[640,2182...|\n",
      "|   2402|Borderlands| Positive|So I spent a coup...|[so, i, spent, a,...|[spent, couple, h...|(262144,[21823,26...|(262144,[21823,26...|\n",
      "|   2402|Borderlands| Positive|So I spent a few ...|[so, i, spent, a,...|[spent, hours, so...|(262144,[21823,45...|(262144,[21823,45...|\n",
      "|   2402|Borderlands| Positive|So I spent a few ...|[so, i, spent, a,...|[spent, hours, ma...|(262144,[640,2182...|(262144,[640,2182...|\n",
      "|   2402|Borderlands| Positive|2010 So I spent a...|[2010, so, i, spe...|[2010, spent, hou...|(262144,[640,2182...|(262144,[640,2182...|\n",
      "|   2402|Borderlands| Positive|                 was|               [was]|                  []|      (262144,[],[])|      (262144,[],[])|\n",
      "|   2403|Borderlands|  Neutral|Rock-Hard La Varl...|[rock-hard, la, v...|[rock-hard, la, v...|(262144,[19937,51...|(262144,[19937,51...|\n",
      "|   2403|Borderlands|  Neutral|Rock-Hard La Varl...|[rock-hard, la, v...|[rock-hard, la, v...|(262144,[19937,51...|(262144,[19937,51...|\n",
      "|   2403|Borderlands|  Neutral|Rock-Hard La Varl...|[rock-hard, la, v...|[rock-hard, la, v...|(262144,[19937,51...|(262144,[19937,51...|\n",
      "|   2403|Borderlands|  Neutral|Rock-Hard La Vita...|[rock-hard, la, v...|[rock-hard, la, v...|(262144,[19937,60...|(262144,[19937,60...|\n",
      "|   2403|Borderlands|  Neutral|Live Rock - Hard ...|[live, rock, -, h...|[live, rock, -, h...|(262144,[2437,988...|(262144,[2437,988...|\n",
      "|   2403|Borderlands|  Neutral|I-Hard like me, R...|[i-hard, like, me...|[i-hard, like, me...|(262144,[27971,63...|(262144,[27971,63...|\n",
      "|   2404|Borderlands| Positive|that was the firs...|[that, was, the, ...|[first, borderlan...|(262144,[16793,11...|(262144,[16793,11...|\n",
      "|   2404|Borderlands| Positive|this was the firs...|[this, was, the, ...|[first, borderlan...|(262144,[110357,1...|(262144,[110357,1...|\n",
      "+-------+-----------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)  \n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    return text\n",
    "\n",
    "preprocess_text_udf = spark.udf.register(\"preprocess_text\", preprocess_text)\n",
    "df = df.withColumn(\"Content\", preprocess_text_udf(\"Content\"))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Content\", outputCol=\"words\")\n",
    "df_words = tokenizer.transform(df)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "df_filtered_words = remover.transform(df_words)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\")\n",
    "featurizedData = hashingTF.transform(df_filtered_words)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Sentiment\", outputCol=\"label\")\n",
    "df_final = indexer.fit(rescaledData).transform(rescaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.866391\n",
      "Best model parameters:\n",
      "regParam: 0.01\n",
      "elasticNetParam: 0.0\n",
      "maxIter: 10\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.01])  \n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \n",
    "             .addGrid(lr.maxIter, [10, 50, 100]) \n",
    "             .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds = 3)  \n",
    "\n",
    "cvModel = crossval.fit(train_data)\n",
    "predictions = cvModel.transform(test_data)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Logistic Regression Accuracy: %f\" % accuracy)\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "print(\"Best model parameters:\")\n",
    "print(\"regParam:\", bestModel._java_obj.getRegParam())\n",
    "print(\"elasticNetParam:\", bestModel._java_obj.getElasticNetParam())\n",
    "print(\"maxIter:\", bestModel._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Accuracy: 0.825080\n",
      "Best model parameters:\n",
      "Smoothing parameter: 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(featuresCol='features', labelCol='label')\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(nb.smoothing, [0.0, 1.0, 2.0])  \n",
    "             .build())\n",
    "\n",
    "crossval = CrossValidator(estimator=nb,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)  \n",
    "\n",
    "cvModel = crossval.fit(train_data)\n",
    "\n",
    "predictions = cvModel.transform(test_data)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Naive Bayes Classifier Accuracy: %f\" % accuracy)\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "print(\"Best model parameters:\")\n",
    "print(\"Smoothing parameter:\", bestModel._java_obj.getSmoothing())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
